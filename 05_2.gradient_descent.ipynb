{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2759, 0.2134, 0.5695],\n",
       "        [0.9993, 0.5222, 0.6267],\n",
       "        [0.4802, 0.7211, 0.1802]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target)\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# You can get gradient of x, after differentiation.\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1152, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0391,  0.0030,  0.0599],\n",
      "        [ 0.1332,  0.0049,  0.0059],\n",
      "        [-0.0489, -0.0175, -0.1600]])\n",
      "1-th Loss: 6.9687e-02\n",
      "tensor([[0.2368, 0.2104, 0.5096],\n",
      "        [0.8661, 0.5172, 0.6207],\n",
      "        [0.5290, 0.7386, 0.3402]], requires_grad=True)\n",
      "tensor([[ 0.0304,  0.0023,  0.0466],\n",
      "        [ 0.1036,  0.0038,  0.0046],\n",
      "        [-0.0380, -0.0136, -0.1244]])\n",
      "2-th Loss: 4.2156e-02\n",
      "tensor([[0.2064, 0.2081, 0.4630],\n",
      "        [0.7625, 0.5134, 0.6161],\n",
      "        [0.5670, 0.7522, 0.4646]], requires_grad=True)\n",
      "tensor([[ 0.0236,  0.0018,  0.0362],\n",
      "        [ 0.0806,  0.0030,  0.0036],\n",
      "        [-0.0296, -0.0106, -0.0968]])\n",
      "3-th Loss: 2.5502e-02\n",
      "tensor([[0.1827, 0.2063, 0.4268],\n",
      "        [0.6820, 0.5104, 0.6125],\n",
      "        [0.5966, 0.7629, 0.5613]], requires_grad=True)\n",
      "tensor([[ 0.0184,  0.0014,  0.0282],\n",
      "        [ 0.0627,  0.0023,  0.0028],\n",
      "        [-0.0230, -0.0083, -0.0753]])\n",
      "4-th Loss: 1.5427e-02\n",
      "tensor([[0.1644, 0.2049, 0.3986],\n",
      "        [0.6193, 0.5081, 0.6098],\n",
      "        [0.6196, 0.7711, 0.6366]], requires_grad=True)\n",
      "tensor([[ 0.0143,  0.0011,  0.0219],\n",
      "        [ 0.0487,  0.0018,  0.0022],\n",
      "        [-0.0179, -0.0064, -0.0585]])\n",
      "5-th Loss: 9.3324e-03\n",
      "tensor([[0.1501, 0.2038, 0.3767],\n",
      "        [0.5706, 0.5063, 0.6076],\n",
      "        [0.6374, 0.7775, 0.6951]], requires_grad=True)\n",
      "tensor([[ 0.0111,  0.0008,  0.0170],\n",
      "        [ 0.0379,  0.0014,  0.0017],\n",
      "        [-0.0139, -0.0050, -0.0455]])\n",
      "6-th Loss: 5.6455e-03\n",
      "tensor([[0.1389, 0.2030, 0.3597],\n",
      "        [0.5327, 0.5049, 0.6059],\n",
      "        [0.6513, 0.7825, 0.7407]], requires_grad=True)\n",
      "tensor([[ 0.0087,  0.0007,  0.0133],\n",
      "        [ 0.0295,  0.0011,  0.0013],\n",
      "        [-0.0108, -0.0039, -0.0354]])\n",
      "7-th Loss: 3.4152e-03\n",
      "tensor([[0.1303, 0.2023, 0.3464],\n",
      "        [0.5032, 0.5038, 0.6046],\n",
      "        [0.6621, 0.7864, 0.7761]], requires_grad=True)\n",
      "tensor([[ 0.0067,  0.0005,  0.0103],\n",
      "        [ 0.0229,  0.0008,  0.0010],\n",
      "        [-0.0084, -0.0030, -0.0275]])\n",
      "8-th Loss: 2.0660e-03\n",
      "tensor([[0.1236, 0.2018, 0.3361],\n",
      "        [0.4803, 0.5030, 0.6036],\n",
      "        [0.6706, 0.7894, 0.8036]], requires_grad=True)\n",
      "tensor([[ 0.0052,  0.0004,  0.0080],\n",
      "        [ 0.0178,  0.0007,  0.0008],\n",
      "        [-0.0065, -0.0023, -0.0214]])\n",
      "9-th Loss: 1.2498e-03\n",
      "tensor([[0.1183, 0.2014, 0.3281],\n",
      "        [0.4624, 0.5023, 0.6028],\n",
      "        [0.6771, 0.7918, 0.8250]], requires_grad=True)\n",
      "tensor([[ 0.0041,  0.0003,  0.0062],\n",
      "        [ 0.0139,  0.0005,  0.0006],\n",
      "        [-0.0051, -0.0018, -0.0167]])\n",
      "10-th Loss: 7.5605e-04\n",
      "tensor([[0.1142, 0.2011, 0.3218],\n",
      "        [0.4486, 0.5018, 0.6022],\n",
      "        [0.6822, 0.7936, 0.8417]], requires_grad=True)\n",
      "tensor([[ 0.0032,  0.0002,  0.0049],\n",
      "        [ 0.0108,  0.0004,  0.0005],\n",
      "        [-0.0040, -0.0014, -0.0130]])\n",
      "11-th Loss: 4.5736e-04\n",
      "tensor([[0.1111, 0.2008, 0.3170],\n",
      "        [0.4378, 0.5014, 0.6017],\n",
      "        [0.6861, 0.7950, 0.8546]], requires_grad=True)\n",
      "tensor([[ 0.0025,  0.0002,  0.0038],\n",
      "        [ 0.0084,  0.0003,  0.0004],\n",
      "        [-0.0031, -0.0011, -0.0101]])\n",
      "12-th Loss: 2.7667e-04\n",
      "tensor([[0.1086, 0.2007, 0.3132],\n",
      "        [0.4294, 0.5011, 0.6013],\n",
      "        [0.6892, 0.7961, 0.8647]], requires_grad=True)\n",
      "tensor([[ 0.0019,  0.0001,  0.0029],\n",
      "        [ 0.0065,  0.0002,  0.0003],\n",
      "        [-0.0024, -0.0009, -0.0078]])\n",
      "13-th Loss: 1.6737e-04\n",
      "tensor([[0.1067, 0.2005, 0.3103],\n",
      "        [0.4228, 0.5008, 0.6010],\n",
      "        [0.6916, 0.7970, 0.8726]], requires_grad=True)\n",
      "tensor([[ 0.0015,  0.0001,  0.0023],\n",
      "        [ 0.0051,  0.0002,  0.0002],\n",
      "        [-0.0019, -0.0007, -0.0061]])\n",
      "14-th Loss: 1.0125e-04\n",
      "tensor([[0.1052, 0.2004, 0.3080],\n",
      "        [0.4178, 0.5007, 0.6008],\n",
      "        [0.6935, 0.7977, 0.8787]], requires_grad=True)\n",
      "tensor([[ 1.1585e-03,  8.7963e-05,  1.7753e-03],\n",
      "        [ 3.9483e-03,  1.4606e-04,  1.7564e-04],\n",
      "        [-1.4483e-03, -5.2003e-04, -4.7422e-03]])\n",
      "15-th Loss: 6.1249e-05\n",
      "tensor([[0.1041, 0.2003, 0.3062],\n",
      "        [0.4138, 0.5005, 0.6006],\n",
      "        [0.6949, 0.7982, 0.8834]], requires_grad=True)\n",
      "tensor([[ 9.0109e-04,  6.8416e-05,  1.3808e-03],\n",
      "        [ 3.0709e-03,  1.1361e-04,  1.3660e-04],\n",
      "        [-1.1265e-03, -4.0446e-04, -3.6884e-03]])\n",
      "16-th Loss: 3.7052e-05\n",
      "tensor([[0.1032, 0.2002, 0.3048],\n",
      "        [0.4107, 0.5004, 0.6005],\n",
      "        [0.6961, 0.7986, 0.8871]], requires_grad=True)\n",
      "tensor([[ 7.0085e-04,  5.3214e-05,  1.0740e-03],\n",
      "        [ 2.3885e-03,  8.8361e-05,  1.0624e-04],\n",
      "        [-8.7614e-04, -3.1458e-04, -2.8687e-03]])\n",
      "17-th Loss: 2.2414e-05\n",
      "tensor([[0.1025, 0.2002, 0.3038],\n",
      "        [0.4084, 0.5003, 0.6004],\n",
      "        [0.6969, 0.7989, 0.8900]], requires_grad=True)\n",
      "tensor([[ 5.4511e-04,  4.1389e-05,  8.3531e-04],\n",
      "        [ 1.8577e-03,  6.8731e-05,  8.2639e-05],\n",
      "        [-6.8144e-04, -2.4467e-04, -2.2312e-03]])\n",
      "18-th Loss: 1.3559e-05\n",
      "tensor([[0.1019, 0.2001, 0.3029],\n",
      "        [0.4065, 0.5002, 0.6003],\n",
      "        [0.6976, 0.7991, 0.8922]], requires_grad=True)\n",
      "tensor([[ 4.2397e-04,  3.2190e-05,  6.4969e-04],\n",
      "        [ 1.4449e-03,  5.3459e-05,  6.4280e-05],\n",
      "        [-5.3000e-04, -1.9030e-04, -1.7354e-03]])\n",
      "19-th Loss: 8.2026e-06\n",
      "tensor([[0.1015, 0.2001, 0.3023],\n",
      "        [0.4051, 0.5002, 0.6002],\n",
      "        [0.6981, 0.7993, 0.8939]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    loss.backward() # Calculate gradients.\n",
    "\n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    # You don't need to aware this now.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
